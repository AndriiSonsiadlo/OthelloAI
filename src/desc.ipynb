{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# OthelloAI\n",
    "\n",
    "### nn.py\n",
    "\n",
    "1. `__init__(self, layer_dims, learning_rate):`\n",
    "   - This is the constructor method for the `NN` class that initializes the parameters for the neural network.\n",
    "   - `layer_dims` is a list of integers representing the number of neurons in each layer of the network.\n",
    "   - `learning_rate` is a float representing the learning rate of the network.\n",
    "   - The method initializes an empty list for `self.layers` and populates it with weight matrices using NumPy's `np.random.normal` method. Each weight matrix is a 2D NumPy array representing the weights connecting two adjacent layers of neurons in the network. The values of the weight matrix are randomly initialized from a normal distribution with mean 0 and standard deviation 1.\n",
    "\n",
    "2. `save(self, filename)`:\n",
    "   - This method saves the trained model to a file using the pickle module. The filename argument is the name of the file to which the model will be saved.\n",
    "\n",
    "3. `load(self, filename)`:\n",
    "   - This method loads a previously trained model from a file using the pickle module. The `filename` argument is the name of the file from which the model will be loaded.\n",
    "4. `mk_vec(self, vector1D, add_bias=True)`:\n",
    "   - This method converts a 1D NumPy array into a 2D column vector with an optional bias term.\n",
    "   - The `vector1D` argument is a 1D NumPy array.\n",
    "   - If `add_bias` is set to `True`, the method adds a bias term of value 1 to the bottom of the column vector.\n",
    "   - The method returns a 2D column vector.\n",
    "\n",
    "5. `get_output(self, input_vector)`:\n",
    "   - This method computes the output of the neural network given an input vector.\n",
    "   - The `input_vector` argument is a 2D column vector representing the input to the network.\n",
    "   - The method applies the weights of each layer to the previous layer's output using the @ operator (matrix multiplication) and passes the result through an activation function specified by the `activation` function (not shown in the code).\n",
    "   - The method returns the final output of the network.\n",
    "\n",
    "6. `back_prop(self, sample, target)`:\n",
    "   - This method performs backpropagation to train the neural network using a single sample and target output.\n",
    "   - The `sample` argument is a 2D column vector representing the input to the network.\n",
    "   - The `target` argument is a 2D column vector representing the desired output of the network.\n",
    "   - The method first computes the outputs of each layer of the network using the `get_output` method and stores them in a list called `outputs`.\n",
    "   - The method then computes the delta values for each layer of the network using the backpropagation algorithm. The delta values represent the error in the network's output and are used to update the weights of the network during training.\n",
    "   - The method updates the weights of the network using the delta values and the learning rate specified in the constructor.\n",
    "   - The method returns the final output of the network."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### players.py\n",
    "\n",
    "Ten kod opisuje klasę RLPlayer, która reprezentuje gracza korzystającego z podejścia uczenia ze wzmocnieniem (reinforcement learning) w grze planszowej. Poniżej znajduje się szczegółowa analiza każdej metody w tej klasie:\n",
    "\n",
    "1. Metoda `__init__(self, q_lr, discount_factor, net_lr=0.01, board_size=8)`:\n",
    "\n",
    "- Jest to metoda konstruktora klasy `RLPlayer`.\n",
    "- Przyjmuje cztery argumenty: `q_lr` - współczynnik uczenia dla algorytmu Q-learning, `discount_factor` - współczynnik dyskontowania dla algorytmu Q-learning, `net_lr` - współczynnik uczenia sieci neuronowej wykorzystywanej do określenia strategii gry, `board_size` - rozmiar planszy do gry (domyślnie 8x8).\n",
    "- Metoda tworzy sieć neuronową `(self.policy_net)` o architekturze `[board_size^2, board_size^2 * 2, board_size^2 * 2, board_size^2, board_size^2]` za pomocą klasy NN (która nie jest dostarczona w tym fragmencie kodu).\n",
    "- Ustawia wartość początkową zmiennej self.epsilon na 0.6, która odpowiada za strategię eksploracji i eksploatacji.\n",
    "- Ustawia wartości początkowe zmiennych self.q_lr, self.discount_factor, self.play_history (tablica przechowująca historię ruchów gracza) i `self.wins` (liczba wygranych gracza).\n",
    "\n",
    "2. Metoda `play(self, place_func, board: Board, board_state, me, log_history=True)`:\n",
    "\n",
    "- Metoda ta odpowiada za wykonanie ruchu przez gracza.\n",
    "- Przyjmuje pięć argumentów: `place_func` - funkcja umożliwiająca wykonanie ruchu przez gracza, `board` - obiekt planszy do gry, `board_state` - stan planszy, `me` - kolor pionków gracza (wartość 1 lub -1), `log_history` - wartość logiczna określająca, czy ruch ma być zapisany w historii ruchów gracza (domyślnie True).\n",
    "- Metoda przekształca stan planszy `board_state` na wektor `input_state` o wymiarze (board_size^2, 1), w którym pionki gracza są oznaczone wartością 1, a pionki przeciwnika wartością -1.\n",
    "- Następnie metoda wykonuje ruch zgodnie ze strategią eksploracji i eksploatacji (epsilon-greedy), losując ruch z prawdopodobieństwem `self.epsilon` lub wybierając ruch z najwyższą wartością akcji zgodnie z siecią neuronową `self.policy_net`.\n",
    "- Po wykonaniu ruchu, metoda zapisuje ruch w historii ruchów gracza, jeśli `log_history=True`, a następnie zwraca wartość logic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### main.py\n",
    "\n",
    "Kod jest używany do szkolenia modelu Reinforcement Learning (RL) dla gry w Othello. Poniżej znajduje się krótka analiza kodu:\n",
    "\n",
    "1. Kod importuje niezbędne biblioteki, takie jak datetime, pprint, numpy i matplotlib.\n",
    "2. Zdefiniowana jest wielkość planszy, rozmiar meczu i liczba epok do szkolenia.\n",
    "3. Inicjalizowany jest obiekt RLPlayer jako gracza i inny obiekt RLPlayer jako przeciwnika, który zostanie użyty w grze.\n",
    "4. Tworzona jest pusta lista `player_wins` w celu śledzenia liczby wygranych gracza w każdej epoce.\n",
    "5. W pętli wykonywana jest nauka modelu i aktualizacja wag dla każdej epoki.\n",
    "6. W ramach pętli resetowany jest licznik wygranych gracza, a wskaźnik eksploracji jest redukowany poprzez zmniejszenie wartości epsilonu.\n",
    "7. Wykonywana jest pętla meczów o rozmiarze `match_size` między graczem a rp, a dla każdej gry inicjalizowana jest nowa gra, uruchamiana i obliczany jest ostateczny wynik.\n",
    "8. Następnie obliczany jest wynik gracza poprzez odjęcie 0,5 od znormalizowanego wyniku i pomnożenie go przez 2. Ma to na celu przeliczenie wyniku na wartość między -1 a 1.\n",
    "9. Aktualizowany jest licznik wygranych gracza, a historia rozgrywki gracza zostaje dodana do listy `player_gameplay_history`.\n",
    "10. Po zakończeniu wszystkich meczów aktualizowane są wagi gracza na podstawie `player_gameplay_history`.\n",
    "11. Ostatecznie wagi modelu są zapisywane w pliku, liczba wygranych dla każdej epoki jest zapisywana w pliku CSV, a liczba wygranych jest wykreslana za pomocą matplotlib.\n",
    "\n",
    "Ogólnie kod ten służy do szkolenia modelu RL za pomocą algorytmu Q-learning i sieci neuronowej, która przyjmuje stan planszy jako wejście i generuje politykę do wykonania dla gracza. Model uczy się grać w Othello, grając przeciwko losowemu przeciwnikowi i aktualizując wagi na podstawie historii rozgrywki.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
